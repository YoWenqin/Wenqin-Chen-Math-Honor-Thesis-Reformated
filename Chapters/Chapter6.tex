% Chapter Template

\chapter{Classification of Entanglement} % Main chapter title

\label{Chapter6-classification of entanglement} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}
\begin{figure}[h]
    \centering
    \includegraphics[scale=0.4]{"actual entanglement".jpg}
    \caption{The First-Ever Photo of Quantum Entanglement captured by physicists at the University of Glasgow. \cite{moreau2019imaging}}
    \label{fig:actual entanglemen}
\end{figure}

When two particles are entangled, their states are so correlated that interacting with one immediately affects the other, even if they are far apart in space.  It is no wonder that Albert Einstein once described entanglement as "spooky action at a distance". While at this point we have defined entanglement (see {\bf{Definition}} \ref{definition: entanglement with state vector} and \textbf{Definition \ref{def: entanglement with density matrix}}) and used its properties (in the E91 Protocol laid out in \textbf{Section \ref{section: e91}}), it is natural to wonder {\emph{exactly}} when a composite state in $\mathbb{C}^n \otimes \mathbb{C}^n$ is entangled.
Are all entangled systems equivalent, or are there some systems that are more entangled than others? In this chapter we answer these questions.


We will begin with a mathematical classification of when a 2-qudit state is a tensor product of two vectors and determine to what extent Alice's vector and Bob's vector are correlated. In the case that Alice's and Bob's vectors are highly correlated, we then examine how Bob can use this correlation to make a one-time pad. 
% We will begin with a mathematical classification of when a vector in a tensor product of two vector spaces is a product of two vectors. We apply this to the situation when Alice and Bob share a $2$-qudit quantum state in $\mathbb{C}^n \otimes \mathbb{C}^n$. When this is the case, we first determine exactly when and to what extent the vectors on Alice's and Bob's side of the tensor product are correlated. If Alice and Bob's side of 
% We then use this idea to 


When Alice and Bob each have a system with state space ${\mathbb{C}}^n$, we know that their composite system will have state space ${\mathbb{C}}^n \otimes {\mathbb{C}}^n={\mathbb{C}}^{n^2}$ by Postulate 4 in \textbf{Chapter \ref{Chapter3-postulates}}. If $\ket{\psi}$ is the state vector for their composite system, the convention is to think of all terms on the left-hand side of the tensor product symbol as representing Alice's system, and all terms on the right-hand side as representing Bob's system.  

In the next section we show that the rank of a matrix associated to a composite state $\ket{\psi}$ measures how entangled Alice's and Bob's systems are.

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
\pagebreak

\section{Rank of Coefficient Matrix and the Strength of Entanglement}

We begin will a purely mathematical proposition which characterizes precisely when a vector in the tensor product of two vector spaces is a product of two vectors.

\begin{prop}
\label{rank prop}
Let $\ket{\psi}$ be a state in $\mathbb{C}^n \otimes \mathbb{C}^n$, and let $S=\{\ket{e_i}\}$ be the standard basis for $\mathbb{C}^n$. Define the coefficient matrix $M(\ket{\psi})$ by
\begin{equation}
M(\ket{\psi})=(a_{i,j}) \textrm{, where}\ket{\psi}=\sum\limits_{i,j}a_{i,j}|e_i e_j\rangle . 
\end{equation}
Also, set $S(\ket{\psi})=\{m\in \mathbb{N}: \ket{\psi}=\sum_{i=1}^m\ket{v_i w_i}, for \ket{v_i}, \ket{w_i} \in \mathbb{C}^n\}$.\\  Then, $rank(M(\ket{\psi}))=\textrm{min} (S(\ket{\psi})$.
\end{prop}

Notice in particular, that if $rank(M(\ket{\psi}))=1$, then there exist certain $\ket{v_1}, \ket{w_1} \in \mathbb{C}^n$ such that $\ket{\psi}=\ket{v_1}\ket{w_1}$.  Thus, we have the following Corollary which classifies entangled states.
\begin{corollary}
If $rank(M(\ket{\psi}))=1$, then the state $\ket{\psi}$ is separable. Otherwise, $\ket{\psi}$ is entangled.
\end{corollary}

While the Proposition and Corollary show that the rank of the coefficient matrix tells us whether $\ket{\psi}$ is entangled we will later see that the rank of a coefficient matrix is also a measure of {\emph{how}} entangled a composite system is. More precisely, we will classify the connection between the rank and the strength of the {\emph{correlation}} between Bob's and Alice's parts of the composite system.

But before we proceed to the rest of the discussion on the connection between rank and entanglement, let's first prove the following Lemma we need to prove \textbf{Proposition \ref{rank prop}}.

\begin{lemma}
\label{independence lemma}
Let $k=min S(\ket{\psi})$, and suppose $\ket{\psi}=\sum\limits_{i=1}^k \ket{v_iw_i}$.  Then, the sets $\{\ket{v_1}, \ket{v_2},...,\ket{v_k}\}$ and $\{\ket{w_1}, \ket{w_2},...,\ket{w_k}\}$ are both linearly independent.
\end{lemma}

\begin{proof}
For a contradiction, let $k, \ket{\psi}, \ket{v_i}, \ket{w_i}$ be as above, and suppose that $\{\ket{v_1}, \ket{v_2},...,\ket{v_k}\}$ is linearly dependent. Then, without loss of generality, say $\ket{v_k} \in span\{\ket{v_1}, \ket{v_2},...\ket{v_{k-1}}\}$. Then, $\ket{v_k}=\sum\limits_{i=1}^{k-1} a_i \ket{v_i}$, for constants $a_i \in \mathbb{C}$, and
\begin{eqnarray*}
\ket{\psi}&=&\sum\limits_{i=1}^{k-1} \ket{v_i w_i} + \ket{v_k w_k}\\
&=&\sum_{i=1}^{k-1} \ket{v_i w_i} + \left(\sum_{i=1}^{k-1} a_i\ket{v_i}\right) \otimes \ket{w_k}\\
&=&\sum_{i=1}^{k-1} \ket{v_i w_i} + \sum_{i=1}^{k-1} a_i\ket{v_iw_k}\\
&=&\sum_{i=1}^{k-1} \ket{v_i} \otimes (\ket{w_i}+a_i\ket{w_k}).\\
\end{eqnarray*}

Therefore $(k-1) \in S(\ket{\psi})$, contradicting the minimality of $k$.  Thus, $\{\ket{v_1}, \ket{v_2},...,\ket{v_k}\}$ and $\{\ket{w_1}, \ket{w_2},...,\ket{w_m}\}$ are linearly independent as required.
\end{proof}

\bigskip
Now we are ready to prove \textbf{Proposition \ref{rank prop}}.

\begin{proof}
Let $k=min(S(\ket{\psi}))$ and say $\ket{\psi}=\sum\limits_{i=1}^k \ket{v_i w_i}$. By Lemma \ref{independence lemma}, $\{\ket{v_1}, \ket{v_2},...,\ket{v_k}\}$ and $\{\ket{w_1}, \ket{w_2},...,\ket{w_k}\}$ are linearly independent.  We now extend each linearly independent set to bases for $\mathbb{C}^n$. Thus, let $B=\{\ket{v_1}, \ket{v_2},...,\ket{v_k}, \ket{v_{k+1}},...,\ket{v_n}\}$ and $B'=\{\ket{w_1}, \ket{w_2},...,\ket{w_n}\}$ be bases, and let $M_{B \otimes B'}(\ket{\psi})$ denote the coefficient matrix of $\ket{\psi}=\sum\limits_{i=1}^{n}\ket{v_i w_i}$ with respect to the basis $B \otimes B'$. That is, $M_{B \otimes B'}(\ket{\psi})$ is the {\emph{coefficient matrix}} for the basis $\{\ket{v_i w_j} \}$ of ${\mathbb{C}}^{n^2}$.  By inspection, 
$$M_{B \otimes B'}(\ket{\psi})=\begin{pmatrix}
\mathbb{I}_k && 0 \\
0 && 0
\end{pmatrix}$$
so clearly $rank(M_{B \otimes B'}(\ket{\psi}))$ is k.

Thus, we need only show that $rank(M_{B \otimes B'}(\ket{\psi}))=rank(M(\ket{\psi})$.  To do this, we will find two invertible $n \times n$ matrices P, Q with 
\begin{equation}
PM_{B \otimes B'}(\ket{\psi})Q^{-1}=M(\ket{\psi}).
\end{equation}
Continuing, let $\ket{v_i}=\sum\limits_{j=1}^n v_{ji}\ket{e_j}$, and
$\ket{w_i}=\sum\limits_{j=1}^n w_{ji}\ket{e_j}$.  Since B and B' are both linearly independent, $P=(v_{i,j})$ and $Q=(w_{i,j})$ are both invertible.
% $P=\begin{pmatrix}
% \vert && \vert && \hdots && \vert\\
% \ket{v_1} && \ket{v_2} && \hdots && \ket{v_n}\\
% \vert && \vert && \hdots && \vert\\
% \end{pmatrix}$,\\
% $Q^{-1}=\begin{pmatrix}
% \text{---} && \ket{w_1}^T &&\text{---} \\
% \text{---} && \ket{w_2}^T &&\text{---} \\
% \vdots && \vdots && \vdots \\
% \text{---} && \ket{w_n}^T &&\text{---} \\
% \end{pmatrix}$.


Then, set $(b_{i,j})=PM_{B \otimes B'}(\ket{\psi})Q^{-1}$.  By direct computation, we have that
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}

=\begin{pmatrix}
\ket{v_1} && \ket{v_2} && \hdots && \ket{v_n}
\end{pmatrix}
\begin{pmatrix}
\text{---} && \ket{w_1}^T &&\text{---} \\
\text{---} && \ket{w_2}^T &&\text{---} \\
\vdots && \vdots && \vdots \\
\text{---} && \ket{w_k}^T &&\text{---} \\
\text{---} && 0 &&\text{---}
\end{pmatrix}\\
=\begin{pmatrix}
v_{11}w_{11}+\hdots+v_{1k}w_{1k} && v_{11}w_{21}+\hdots+v_{1k}w_{2k} && \hdots && v_{11}w_{n1}+\hdots+v_{1k}w_{nk}\\
v_{21}w_{11}+\hdots+v_{2k}w_{1k} && v_{21}w_{21}+\hdots+v_{2k}w_{2k} && \hdots && v_{21}w_{n1}+\hdots+v_{2k}w_{nk}\\
\vdots && \vdots && \vdots && \vdots\\
v_{n1}w_{11}+\hdots+v_{nk}w_{1k} && v_{n1}w_{21}+\hdots+v_{nk}w_{2k} && \hdots && v_{n1}w_{n1}+\hdots+v_{nk}w_{nk}
\end{pmatrix}

\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{gather*}
    b_{ij}=\sum_{t=1}^k v_{it}w_{jt}
\end{gather*}


On the other hand,
\begin{eqnarray*}
\ket{\psi}&=&\sum_{i=1}^k \ket{v_i w_i}\\
&=&\sum_{i=1}^k \left(\sum_{j=1}^{n} v_{ji} \ket{e_j}) \otimes (\sum_{s=1}^n w_{si}\ket{e_s}\right)\\
&=&\sum_{i=1}^k \sum_{j=1}^{n} \sum_{s=1}^n v_{ji} w_{si} \ket{e_j e_s}\\
\end{eqnarray*}

\noindent
Thus, if $M(\ket{\psi})=(a_{lr})$, then $a_{lr}=\sum_{i=1}^k v_{lk} w_{rk}=b_{lr}$.
So $PM_{B \otimes B'}(\ket{\psi})Q^{-1}=M(\ket{\psi})$.  Therefore, $rank(M(\ket{\psi}))=rank(M_{B \otimes B'}(\ket{\psi}))=k$.
\end{proof}

This established {\bf{Proposition}} \ref{rank prop}. We include an example of the computation above done in the case when $k=2, n=3$.
\begin{example}
Consider the case where $k=2, n=3$.

Then 
$$M_{B \otimes B'}(\ket{\psi})=\begin{pmatrix}
\mathbb{I}_2 && 0 \\
0 && 0
\end{pmatrix}$$.

We want to show $rank(M_{B \otimes B'}(\ket{\psi}))=rank(M(\ket{\psi})=2$ by finding two $3\  \times 3$ invertible matrices P,Q such that $PM_{B \otimes B'}(\ket{\psi})Q^{-1}=M(\ket{\psi})$.
Let
\begin{gather*}
\ket{v_i}=\sum_{j=1}^3 v_{ji}\ket{e_j}\\
\ket{w_i}=\sum_{j=1}^3 w_{ji}\ket{e_j}\\
P=(\ket{v_1}, \ket{v_2}, \ket{v_3})\\
Q^{-1}=(\ket{w_1}, \ket{w_2}, \ket{w_3})^T
\end{gather*}

Then
\begin{eqnarray*}
P M_{B \otimes B'}(\ket{\psi}) Q^{-1}&=&\begin{pmatrix}
v_{11} && v_{12} && v_{13}\\
v_{21} && v_{22} && v_{23}\\
v_{31} && v_{32} && v_{33}
\end{pmatrix}
\begin{pmatrix}
1 && 0 && 0\\
0 && 1 && 0\\
0 && 0 && 0\\
\end{pmatrix}
\begin{pmatrix}
w_{11} && w_{21} && w_{31}\\
w_{12} && w_{22} && w_{32}\\
w_{13} && w_{23} && w_{33}
\end{pmatrix}\\
&=&\begin{pmatrix}
v_{11} && v_{12} && 0\\
v_{21} && v_{22} && 0\\
v_{31} && v_{32} && 0
\end{pmatrix}
\begin{pmatrix}
w_{11} && w_{21} && w_{31}\\
w_{12} && w_{22} && w_{32}\\
w_{13} && w_{23} && w_{33}
\end{pmatrix}\\
&=&\begin{pmatrix}
v_{11}w_{11}+v_{12}w_{12} && v_{11}w_{21}+v_{12}w_{22} && v_{11}w_{31}+v_{12}w_{32}\\
v_{21}w_{11}+v_{22}w_{12} && v_{21}w_{21}+v_{22}w_{22} && v_{21}w_{31}+v_{22}w_{32} \\
v_{31}w_{11}+v_{32}w_{12} && v_{31}w_{21}+v_{32}w_{22} && v_{31}w_{31}+v_{32}w_{32} \\
\end{pmatrix}.
\end{eqnarray*}


% \ket{v_i}=\sum_{j=1}^3 v_{ji}\ket{e_j}\\
% \ket{w_i}=\sum_{j=1}^3 w_{ji}\ket{e_j}\\

We also have
\begin{eqnarray*}
\ket{\psi}&=&\ket{v_1 w_1}+\ket{v_2 w_2}\\
&=& (v_{11}\ket{e_1}+v_{21}\ket{e_2}+v_{31}\ket{e_3}) \otimes (w_{11}\ket{e_1}+w_{21}\ket{e_2}+w_{31}\ket{e_3}) \\
&+& (v_{12}\ket{e_1}+v_{22}\ket{e_2}+v_{32}\ket{e_3}) \otimes (w_{12}\ket{e_1}+w_{22}\ket{e_2}+w_{32}\ket{e_1}).
\end{eqnarray*}
Therefore,
\begin{equation}
M(\ket{\psi})=
\begin{pmatrix}
v_{11}w_{11}+v_{12}w_{12} && v_{11}w_{21}+v_{12}w_{22} && v_{11}w_{31}+v_{12}w_{32}\\
v_{21}w_{11}+v_{22}w_{12} && v_{21}w_{21}+v_{22}w_{22} && v_{21}w_{31}+v_{22}w_{32} \\
v_{31}w_{11}+v_{32}w_{12} && v_{31}w_{21}+v_{32}w_{22} && v_{31}w_{31}+v_{32}w_{32} 
\end{pmatrix},
\end{equation}
so 
$$PM_{B \otimes B'}(\ket{\psi})Q^{-1}=M(\ket{\psi}).$$
Thus, we have established that $rank(M(\ket{\psi}))=rank(M_{B \otimes B'}(\ket{\psi}))=2$.
\end{example}

\textcolor{blue}{photons not particles right?}\textcolor{green}{What are you referring to? Are you referring to what we get as we fire a laser beam through a crystal? }

Out task is to now show our this mathematical description of entanglement is consistent with the idea of "spooky action at a distance." In the 1930s Albert Einstein was unhappy with the notion that an event at one point in the universe could influence another event far away. Consider a real world situation where entanglement occurs, for example, when a laser beam is fired through a special kind of crystal to form a pair of entangled photons. Once either one of the photons is measured (as introduced in \textbf{Section \ref{subsection:projective measurement}}), the state of the entire entangled system {\emph{collapses}}.  In particular, the state of the other photon collapses as well, {\emph{even though it may be very far away.}} To reconcile this, Einstein theorized that the two photons contained some information that was hidden from us until the moment of measurement. While this "hidden information" idea ultimately was proven wrong by John Bell (see \textbf{Section \ref{section: bell-nonlocality}}), there is nonetheless a correlation between entangled photons. In other words, even though Alice's photon may be at Smith College while Bob's is on Mars, information about Alice's photon {\emph{is}} information about Bob's. Therefore, our next goal is to show that our "rank test" for entanglement can capture the correlation between the component states of composite system after measurement.

We now define the strength of entanglement of a composite system in terms of the coefficient matrix, and show that our notion is consistent with the degree to which the post measurement states in a composite system are correlated.
% If the rank being 1 implies the joint quantum state is not entangled, what if the coefficent matrix has full rank? We now may make the following definition and proceed to classify the strength of entanglement in terms of the rank of the coefficient matrix. Here by strength, we mean how much information Bob has about Alice once Alice measures on her part of the composite system.

\begin{definition} \label{def: maximally entangled}
 Let $\ket{\psi}$ be a state in ${\mathbb{C}}^{(n^2)}$.  Then, the entanglement degree of $\ket{\psi}$ is $rank(M(\ket{\psi}))$.  If the entanglement degree of $\ket{\psi}$ is $n$, we say $\ket{\psi}$ is maximally entangled \footnote{In some other literature, being maximally entangled means something different: A maximally entangled state is a quantum state which has maximum von Neumann entropy for each bipartition. This is equivalent to saying that the reduced density matrix $\rho_A, \rho_B$ are both a multiple of the identity matrix}.
\end{definition}


We now describe the composite system analogue of the situation described in \textbf{Example \ref{example: orthogonal states for measurements}} and \textbf{Example \ref{example: non-orthogonal states for measurements}}, where Bob tries to determine a state which Alice picks from a known basis by measuring the state Alice picks. 

\textcolor{green}{Our task now is slightly different aside from changing from a single-qudit system to a 2-qudit composite system.} Now, instead of Alice {\emph{selecting}} an element of a basis, she {\emph{measures}} \textcolor{green}{measures her part of} the composite state she shares with Bob (see \textbf{Figure \ref{fig:entanglement}}). \textcolor{green}{Like before, the goal is for Bob to guess the end state that Alice obtains (after her measurement) with certainty. Another similarity with the previous scenario is that Bob is aware of the basis Alice uses for measurement.}

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.8]{"entangled to alice and bob".png}
    \caption{Alice and Bob sharing an entangled state. \textcolor{green}{Say Alice measures with respect to an orthonormal basis $\{\varphi_i\}$ and obtains an end state $\ket{\varphi}$ and the post-measurement state for Bob after Alice's measurement is $\ket{\theta}$, our task is to determine when are Bob able to make use of his $\ket{\theta}$ to infer Alice's end state $\ket{\varphi}$.}}
    \label{fig:entanglement}
\end{figure}
\textcolor{green}{There are two steps or two preconditions that enable Bob to infer Alice's measurement result with certainty.
\begin{enumerate}
    \item After Alice's measurement yet before Bob's measurement, there must exist a correlation between Alice's measurement end state $\ket{\varphi}$ and the post-measurement state of Bob's system $\ket{\theta}$.
    \item If the correlation is strong, Bob must be able to make use of the correlation by measuring with respect to an orthonormal basis that allows him to infer Alice's outcome.
\end{enumerate}
Let's address the first step in this section and address the second one in the next section.}
It's important to remember that in step 1, {\emph{only Alice}} measures her part of their shared state with respect to an orthonormal basis.  Still, because of this measurement, the state of their composite system collapses.  
We are interested
in the situation when Alice measures with respect to an orthonormal basis $\{\ket{\varphi_i}\}$.  If Alice measures with respect to this basis, we know that for some $i_0$, she observes outcome $i_0$, in which case her system has an end state of $\ket{\varphi_{i_0}}$. Using density matrices, this corresponds to a situation \textcolor{green}{we do an operation }
% where Alice measures with respect to
$\ket{\varphi_{i_0}}\bra{\varphi_{i_0}} \otimes I$ \textcolor{green}{on the composite system consisting of Alice and Bob}.  In our first Proposition, we consider only the situation when Alice's system yields state vector $\ket{\varphi}$ post measurement.  This corresponds to the situation where Alice is measuring with respect to {\emph{any}} orthonormal basis that contains $\ket{\varphi}$ as an element. Our task is to determine how correlated Bob's part of their shared state is with Alice's after she has measured.  


\textcolor{blue}{We have to make sure enough is said about this version of the measurement computation.  I've changed some of them, but we can't call a state an "outcome".  Alice observes some outcome and the post measurement state is psi.  Also, in the Proofs below, make sure we don't use any notation or computations that we haven't defined or shown}

\begin{prop}
\label{entanglement-rank}
Let $\ket{\psi} \in \mathbb{C}^n \otimes \mathbb{C}^{n}$ be a quantum state representing a composite system of Alice and Bob. Say Alice first makes a measurement and observes an outcome with end state $\ket{\varphi}\bra{\varphi}$. Let the state of Bob's system after Alice has performed the measurement be denoted by $\ket{\theta}\bra{\theta}$. Then,
\begin{enumerate}
    \item $rank(M(\ket{\psi}))=1$ if and only if $\ket{\theta}\bra{\theta}$ is a constant matrix that doesn't depend on $\ket{\varphi}$. In this situation, the state of Bob's system \textcolor{green}{has no correlation with Alice's measurement end state and} contains no information about Alice.
    \item $rank(M(\ket{\psi}))=n$ if and only if $\ket{\psi}$ is maximally entangled. In this situation, the state of Bob's system \textcolor{green}{has perfect correlation with Alice's measurement end state and} contains perfect information about Alice.
    \item If $1<rank(M(\ket{\psi}))<n$, then the state of Bob's system \textcolor{green}{has partial correlation with Alice's measurement end state and} only contains some information about Alice.
\end{enumerate}
\end{prop}

This means that if the rank is $n$, that what is on Bob's side of the tensor product is perfectly correlated with what is on Alice's side.  If their states are not entangled, there is no correlation at all between their end states.  On the other hand, if their joint system is entangled, but not maximally, there is some (but not perfect) correlation.  

First, let's prove the following lemma.



% We now consider the use of the shared state $|X\rangle$ in message transmission.  Our goal is to classify exactly to what extent information can be transmitted remotely between two parties by use of shared states. The following lemma will tell us what happens in this situation when one of Alice or Bob observes a state $|\psi\rangle$.

\begin{lemma}
\label{end state lemma}
Let $\ket{\psi} \in \mathbb{C}^n \otimes \mathbb{C}^{n}$ be a quantum state representing a composite system of Alice and Bob.  The shared density matrix is therefore $\rho = \ket{\psi}\bra{\psi}$. Then,
\begin{enumerate}
\item Say Alice makes a measurement and observes $\ket{\varphi}\bra{\varphi}$, then Bob's post-measurement state is $\ket{\theta}\bra{\theta}$, where $\ket{\theta}=\frac{1}{\sqrt{C}}M(\ket{\psi})^T \cdot \overline{\ket{\varphi}}$, where $C$ is a constant.
\item Similarly, if Bob makes a measurement and observes observes $\ket{\varphi}\bra{\varphi}$, then Alice's post-measurement state is $\ket{\theta}\bra{\theta}$, where $\ket{\theta}=\frac{1}{\sqrt{C}}M(\ket{\psi})^T\cdot \overline{\ket{\varphi}}$, where $C$ is a constant.\\

\end{enumerate}
\end{lemma}

This Lemma says that when Alice's post-measurement vector is $\ket{\varphi}$, then Bob's is given by left multiplication by $M(\ket{\psi})^T$ up to a global phase and taking conjugates.

\begin{proof}
First we compute the shared density matrix in terms of the standard matrix.  Let $\ket{\psi}$ be given by 
\begin{equation}
\ket{\psi}=\icol{a_1\\ \vdots \\a_{n^2}}.
\end{equation}
Then, 

\begin{equation}
\rho=\ket{\psi}\bra{\psi}=(\sum_{i,j} a_{ij}\ket{e_i e_j}(\sum_{i',j'} \overline{a_{i'j'}}\bra{e_{i'} e_{j'}})=\sum_{i,j} \sum_{i',j'} a_{ij} \overline{a_{i'j'}}\ket{e_i e_j}\bra{e_{i'} e_{j'}}   
\end{equation}

Now say
\begin{equation}
\ket{\varphi}=\icol{b_1\\ \vdots \\b_n}.
\end{equation}
Say Alice observes $\ket{\varphi}\bra{\varphi}_A$.
Then the post-measurement state conditioned on obtaining the outcome $\ket{\varphi}_A$ is
\begin{equation}
\rho_{\ket{\psi}_A}^{AB}=\frac{(\ket{\varphi}\bra{\varphi}_A \otimes I_B)\rho_{AB}(\ket{\varphi}\bra{\varphi}_A \otimes I_B)}{tr((\ket{\varphi}\bra{\varphi}_A \otimes I_B)\rho_{AB})}
\end{equation}

The denominator is just a scalar. Call it C. Then 
\begin{eqnarray}
\rho_{\ket{\psi}_A}^{AB}&=&\frac{1}{C} \sum_{i,j} \sum_{{i'}, {j'}} a_{ij} \overline{a_{i'j'}}\ket{\psi}\braket{\psi|e_i}\braket{e_{i'}|\psi}\bra{\psi}_A \otimes \ket{e_j}\bra{e_{j'}}_B\\
&=&\frac{1}{C} \sum_{i,j} \sum_{{i'}, {j'}} a_{ij} \overline{a_{i'j'}}\overline{b_i}b_{i'}\ket{e_j}\bra{e_{j'}}_B\\
&=&\ket{\varphi}\bra{\varphi}_A \otimes \frac{1}{c} \sum_{i,j} \sum_{{i'}, {j'}} a_{ij} \overline{a_{i'j'}}\overline{b_i}b_{i'}\ket{e_j}\bra{e_{j'}}
\end{eqnarray}

So the post-measurement state for Bob is
\begin{equation}
    \ket{\theta}\bra{\theta}=\frac{1}{C} \sum_{i,j} \sum_{{i'}, {j'}} a_{ij} \overline{a_{i'j'}}\overline{b_i}b_{i'}\ket{e_j}\bra{e_{j'}} 
\end{equation}
This implies that 
\begin{equation}
    \ket{\theta}=\frac{1}{\sqrt{C}} M(\ket{\psi_i})^T \overline{\ket{\varphi}},
    \text{where } \overline{\ket{\varphi}}=\icol{\overline{b_1}\\ \vdots \\ \overline{b_n}}
\end{equation}


Similarly, say Bob observes $\ket{\varphi}\bra{\varphi}$. Then the post-measurement state conditioned on Bob obtaining the outcome $\ket{\psi}_B$ is $\rho_{\ket{\psi}_B}^{AB}=\frac{(I_A \otimes \ket{\psi}\bra{\psi}_B  )\rho_{AB}(I_A \otimes \ket{\psi}\bra{\psi}_B)}{tr((I_A \otimes \ket{\psi}\bra{\psi}_B)\rho_{AB})}$.
Then Alice's post-measurement state is $\ket{\theta}=\frac{1}{\sqrt{C}} M(\ket{\psi_i})^T \overline{\ket{\varphi}}$.
\end{proof}

\bigskip
Now we are ready to prove \textbf{Proposition \ref{entanglement-rank}}.

\textcolor{blue}{The proof below should include a more explicit analysis of Bob trying to "invert" to find Alice's end state. In the case where the rank is m, we have a statement about which vectors can be solved for which shows that the correlation is partial, exactly when m is less than n.  Also, for final editing, this proof, as well as the proof above, should look a little better i.e. more spacing, more use of equation etc.  We should also get rid of implication arrows and write words instead.  First suppose...By the way, maybe there's almost nothing to write in the rank 1 if and only if constant part.  We already know when two vectors have the same overlay} \textcolor{green}{what do you mean by a more explicit analysis of Bob trying to "invert" to find Alice's end state?}


\begin{proof}
Let $M(\ket{\psi})=
\begin{pmatrix}
\vert && \vert && \hdots && \vert\\
\ket{v_1} && \ket{v_2} && \hdots && \ket{v_n}\\
\vert && \vert && \hdots && \vert\\
\end{pmatrix}$\\
Suppose $rank(M(\ket{\psi}))=m$.\\
Without loss of generality, let $\ket{v_{m+1}}, \ket{v_{m+2}},...,\ket{v_n} \in span(\ket{v_1},\ket{v_2},...,\ket{v_m})$, where $\ket{v_1}, ...\ket{v_m} $ are linearly independent.\\
Then there exist scalar sets 
\begin{gather}
\{\lambda_{(m+1),1}, \lambda_{(m+1),2},..., \lambda_{(m+1),m}\},\\
\{\lambda_{(m+2),1}, \lambda_{(m+2),2},..., \lambda_{(m+2),m}\},\\
\vdots\\
\{\lambda_{n,1}, \lambda_{n,2},..., \lambda_{n,m}\}    \end{gather}
 such that $\ket{v_{m+1}}=\sum_{i=1}^m \lambda_{(m+1), i}\ket{v_i},...,\ket{v_n}=\sum_{i=1}^m \lambda_{n, i}\ket{v_i}$. Then

\begin{equation}
M(\ket{\psi})^T=\begin{pmatrix}
\text{---} && \ket{v_1}^T &&\text{---} \\
\vdots && \vdots && \vdots\\
\text{---} && \ket{v_m}^T && \text{---} \\
\text{---} && \sum_{i=1}^m \lambda_{(m+1), i}\ket{v_i}^T  &&\text{---} \\
\vdots && \vdots && \vdots \\
\text{---} && \sum_{i=1}^m \lambda_{n, i}\ket{v_i}^T &&\text{---} \\
\end{pmatrix}    
\end{equation}

When Alice observes $\ket{\psi}$, Bob is essentially trying to solve for the equation\\
\begin{equation}
\ket{\theta}= \frac{1}{\sqrt{C}}\begin{pmatrix}
\text{---} && \ket{v_1}^T &&\text{---} \\
\vdots && \vdots && \vdots\\
\text{---} && \ket{v_m}^T && \text{---} \\
\text{---} && \sum_{i=1}^m \lambda_{(m+1), i}\ket{v_i}^T  &&\text{---} \\
\vdots && \vdots && \vdots \\
\text{---} && \sum_{i=1}^m \lambda_{n, i}\ket{v_i}^T &&\text{---} \\
\end{pmatrix}\overline{\ket{\psi}}  
\end{equation}
Apparently, there are $n-m$ free variables in $\overline{\ket{\psi}}$. This implies that the second statement and the third statement in \textbf{Proposition \ref{entanglement-rank}} are correct.

\bigskip
Let's prove the first statement in \textbf{Proposition \ref{entanglement-rank}}.

$(\Rightarrow):$ When $m=1$, 
\begin{equation}
\ket{\theta}=\frac{1}{\sqrt{C}}\begin{pmatrix}
\text{---} && \ket{v_1}^T &&\text{---} \\
\text{---} && \lambda_2\ket{v_1}^T &&\text{---} \\
\vdots && \vdots && \vdots\\
\text{---} && \lambda_n\ket{v_1}^T &&\text{---} \\
\end{pmatrix}
\overline{\ket{\psi}}\\
=\frac{1}{\sqrt{C}}\begin{pmatrix}
\braket{\psi|v_1}\\
\lambda_2 \braket{\psi|v_1}\\
\vdots\\
\lambda_n \braket{\psi|v_1}
\end{pmatrix}
\end{equation}

Notice $C=(1+\lambda_2^2+\hdots+\lambda_n^2)n(\braket{\psi|v_1})^2$. So\\
\begin{equation}
\ket{\theta}=\frac{1}{\sqrt{n(1+\lambda_2^2+\hdots+\lambda_n^2)}\braket{\psi|v_1}}
\begin{pmatrix}
\braket{\psi|v_1}\\
\lambda_2 \braket{\psi|v_1}\\
\vdots\\
\lambda_n \braket{\psi|v_1}
\end{pmatrix}\\
=\begin{pmatrix}
\frac{1}{\sqrt{n(1+\lambda_2^2+\hdots+\lambda_n^2)}}\\
\frac{\lambda_2}{\sqrt{n(1+\lambda_2^2+\hdots+\lambda_n^2)}}\\
\vdots\\
\frac{\lambda_n}{\sqrt{n(1+\lambda_2^2+\hdots+\lambda_n^2)}}
\end{pmatrix} 
\end{equation}
which is a constant vector that does not depend on $\ket{\psi}$.

\bigskip
$(\Leftarrow):$ Suppose $\ket{\theta}\bra{\theta}$ is a constant matrix that doesn't depend on $\ket{\psi}$. This means for an arbitrary $\ket{\psi}$, the normalized $M(\ket{\psi})^T \overline{\ket{\psi}}$ should always be equal to $\ket{\theta}$.

For contradiction, assume $rank(M(\ket{\psi})) \ge 2$. Let $\ket{\psi}=\ket{e_1}$. Then
\begin{equation}
\ket{\theta} = \frac{1}{\sqrt{C_1}}
\begin{pmatrix}
\text{---} && \ket{v_1}^T &&\text{---} \\
\vdots && \vdots && \vdots\\
\text{---} && \ket{v_m}^T && \text{---} \\
\text{---} && \sum_{i=1}^m \lambda_{(m+1), i}\ket{v_i}^T  &&\text{---} \\
\vdots && \vdots && \vdots \\
\text{---} && \sum_{i=1}^m \lambda_{n, i}\ket{v_i}^T &&\text{---} \\
\end{pmatrix}\ket{e_1}
=\frac{1}{\sqrt{C_1}}
\begin{pmatrix}
v_{11}\\
v_{12}\\
\vdots\\
v_{1m}\\
\sum_{i=1}^m \lambda_{(m+1), i} v_{1i}\\
\vdots\\
\sum_{i=1}^m \lambda_{n, i} v_{1i}
\end{pmatrix}\\
=\begin{pmatrix}
\frac{1}{\sqrt{C_1}}v_{11}\\
\frac{1}{\sqrt{C_1}}v_{12}\\
\vdots\\
\frac{1}{\sqrt{C_1}}v_{1m}\\
\frac{1}{\sqrt{C_1}}\sum_{i=1}^m \lambda_{(m+1), i} v_{1i}\\
\vdots\\
\frac{1}{\sqrt{C_1}}\sum_{i=1}^m \lambda_{n, i} v_{1i}
\end{pmatrix}    
\end{equation}

Let $\ket{\psi}=\ket{e_2}$. Then
$\ket{\theta}=
\begin{pmatrix}
\frac{1}{\sqrt{C_2}}v_{21}\\
\frac{1}{\sqrt{C_2}}v_{22}\\
\vdots\\
\frac{1}{\sqrt{C_2}}v_{2m}\\
\frac{1}{\sqrt{C_2}}\sum_{i=1}^m \lambda_{(m+1), i} v_{2i}\\
\vdots\\
\frac{1}{\sqrt{C_2}}\sum_{i=1}^m \lambda_{n, i} v_{2i}
\end{pmatrix}$.

Repeat the process until $\ket{\psi}=\ket{e_n}$. Then we get\\
$\ket{\theta}
=\begin{pmatrix}
\frac{1}{\sqrt{C_1}}v_{11}\\
\frac{1}{\sqrt{C_1}}v_{12}\\
\vdots\\
\frac{1}{\sqrt{C_1}}v_{1m}\\
\frac{1}{\sqrt{C_1}}\sum_{i=1}^m \lambda_{(m+1), i} v_{1i}\\
\vdots\\
\frac{1}{\sqrt{C_1}}\sum_{i=1}^m \lambda_{n, i} v_{1i}
\end{pmatrix}
=\begin{pmatrix}
\frac{1}{\sqrt{C_2}}v_{21}\\
\frac{1}{\sqrt{C_2}}v_{22}\\
\vdots\\
\frac{1}{\sqrt{C_2}}v_{2m}\\
\frac{1}{\sqrt{C_2}}\sum_{i=1}^m \lambda_{(m+1), i} v_{2i}\\
\vdots\\
\frac{1}{\sqrt{C_2}}\sum_{i=1}^m \lambda_{n, i} v_{2i}
\end{pmatrix}
=\hdots
=\begin{pmatrix}
\frac{1}{\sqrt{C_n}}v_{n1}\\
\frac{1}{\sqrt{C_n}}v_{n2}\\
\vdots\\
\frac{1}{\sqrt{C_n}}v_{nm}\\
\frac{1}{\sqrt{C_n}}\sum_{i=1}^m \lambda_{(m+1), i} v_{ni}\\
\vdots\\
\frac{1}{\sqrt{C_n}}\sum_{i=1}^m \lambda_{n, i} v_{ni}
\end{pmatrix}$\\
So $v_{11}:v_{21}:\hdots:v_{n1}=v_{12}:v_{22}:\hdots:v_{n2}=\hdots=v_{1m}:v_{2m}:\hdots:v_{nm}$.\\
Therefore $\{\ket{v_1}, \ket{v_2},\hdots, \ket{v_m}\}$ is linearly dependent.\\
So $rank(M(\ket{\psi})) = 1$. Contradiction!\\
So when $\ket{\theta}$ is non-constant, the rank must be at least 2.\\
\end{proof}

We illustrate this correlation with some examples.
\textcolor{blue}{Actually, we probably need two examples here.  Make sure to use the same language when you describe each of them.  In these examples, we don't say anything about Bob measuring.  We only point out the correlation between Bob's side and Alice's side.  We can continue with the same examples in a subsequent section}

\begin{example}
\label{example rank full not orth}
Include example where the n=3=rank, but the matrix is not a scalar multiple of a Unitary
\end{example}

\begin{example}
\label{example rank not full}
Say Alice and Bob share a state $\ket{\psi}=\frac{1}{2\sqrt{2}}(\ket{e_1 e_1}+\ket{e_2 e_3}+\ket{e_3 e_1}+\ket{e_3 e_3}$). Then Alice makes a measurement and yields $\ket{\varphi}=\icol{a\\b\\c}$.

Then from \textbf{Lemma \ref{end state lemma}}, Bob's post-measurement state is $\ket{\theta}$,
\begin{equation}
\ket{\theta}=\frac{1}{\sqrt{C}}\begin{pmatrix}
1 && 0 && 1\\
0 && 0 && 0\\
0 && 1 && 1\\
\end{pmatrix}\overline{\ket{\varphi}}=\frac{1}{\sqrt{C}}\begin{pmatrix}
\overline{a+c}\\
0\\
\overline{b+c}
\end{pmatrix}
\end{equation}
Say $\ket{\varphi}=\icol{1\\0\\0}$. Then $\ket{\theta}=\frac{1}{\sqrt{C}}\icol{1\\0\\0}$.

Now consider if Alice makes a measurement and yields $\ket{\varphi'}=\icol{0\\-1\\1}$. Then Bob's post-measurement state is $\ket{\theta'}\bra{\theta'}$, where $\ket{\theta'}=\frac{1}{\sqrt{C}}\icol{1\\0\\0}=\ket{\theta}$.

Of course, we can use Gram-Schmidt to construct an orthonormal basis for $\mathbb{C}^3$ that contains both $\ket{\varphi}$ and $\ket{\varphi'}$. What this means is that for a coefficient matrix whose rank is neither 1 or the full rank, there exists some orthonormal basis that Alice can choose to measure on such that Bob will not be able to infer Alice's measurement result even if he knows the basis Alice chooses. Alice yielding $\ket{\varphi}$ and $\ket{\varphi'}$ will make no difference to what Bob observes on his side at all!

One interesting thing to notice here is that such phenomenon does not occur for an arbitrary orthonormal basis that Alice picks. For example, here's what happens if Alice picks the standard basis to do her measurement. When Alice's measurement outcome is $\ket{e_1}$,$\ket{e_2}$, or $\ket{e_3}$, the post-measurement state for Bob is be $\ket{e_1}$, $\ket{e_3}$, $\ket{e_1}+\ket{e_3}$ respectively. There is this one-to-one mapping form Alice's measurement outcome to Bob's post-measurement state, so Bob will be able to infer Alice's measurement result once he makes a measurement on the standard basis 
\textcolor{green}{wait. Bob will never be able to make a measurement and observe the outcome $\ket{e_1}+\ket{e_3}$ because he's measuring on the standard basis. in other words, if the post-measurement state for Bob is $\ket{e_1}+\ket{e_3}$, he will get the come $\ket{e_1}$ and $\ket{e_3}$ half of the time each. } \textcolor{red}{we are not measuring yet, but we can come back to this example in a later section. Here we are just talking about the correlation. "it might seem that there is a perfect correlation...yet Bob won't be able to make a measurement in this case".}
% \textcolor{green}{how well Bob can do in determining what Alice measured in this case?}
% $\ket{\varphi_2}=$.

% Say $\ket{\theta}=\icol{\theta_1 \\ \theta_2 \\ \theta_3}$, then Bob is trying to solve for
% \begin{equation}
% \begin{cases}
% \sqrt{C}\theta_1=\overline{a+c}\\
% \sqrt{C}\theta_2=0\\
% \sqrt{C}\theta_3=\overline{b+c}
% \end{cases}
% \Rightarrow 
% \begin{cases}
% b=a+\sqrt{C}(\overline{\theta_3}-\overline{\theta_1})\\
% \theta_2=0\\
% c=\sqrt{C}\overline{\theta_1}-a
% \end{cases}
% \end{equation}
% Now consider $\ket{\varphi'}=\begin{pmatrix}
% a'\\a'+\sqrt{C}(\overline{\theta_3}-\overline{\theta_1})\\c'
% \end{pmatrix}$. For $\ket{\varphi}$ and $\ket{\varphi'}$ to be orthogonal,
% \begin{gather}
%     aa'+b(a'+\sqrt{C}(\overline{\theta_3}-\overline{\theta_1}))+cc'=0\\
%     \Rightarrow c'=-\frac{1}{c}(aa'+b(a'+\sqrt{C}(\overline{\theta_3}-\overline{\theta_1})))
% \end{gather}
% So $\ket{\varphi'}=\begin{pmatrix}
% a'\\
% a'+\sqrt{C}(\overline{\theta_3}-\overline{\theta_1})\\
% -\frac{1}{c}(aa'+b(a'+\sqrt{C}(\overline{\theta_3}-\overline{\theta_1})))
% \end{pmatrix}$

% Now check what happens if Alice's measurement yields $\ket{\varphi'}$. Then Bob's post-measurement state is
% \begin{equation}
%     \frac{1}{\sqrt{C}}\begin{pmatrix}
%     \overline{a'-\frac{1}{c}(aa'+b(a'+\sqrt{C}(\overline{\theta_3}-\overline{\theta_1})))}\\
%     0\\
%     \overline{a'+\sqrt{C}(\overline{\theta_3}-\overline{\theta_1})-\frac{1}{c}(aa'+b(a'+\sqrt{C}(\overline{\theta_3}-\overline{\theta_1})))}
%     \end{pmatrix}
% \end{equation}

\end{example}
\textcolor{green}{it's about the worst correlation. if we just pick the standard basis it still looks maximally correlated, but there exists "an" orthonormal basis such that two differents elements in the basis get mapped to the same theta. the rank is about the "worst" it can be. this might be put into the further discussion session}



%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------


Next, we take care of "old business" by making sure that Bob's post-measurement state in \textbf{Lemma \ref{end state lemma}} is {\emph{legal}}.  The point is that a priori the conjugate of Alice's post-measurement state could be in the kernel of the transpose of the coefficient matrix $M(\ket{\psi})$. If this were the case, {\bf{Lemma}} \ref{end state lemma} tells us that the post-measurement state would be zero, which is in fact not a valid quantum state.  Thus, we now show that this occurs with probability zero.

\textcolor{blue}{In this proof, change outcome to end state and make the structure of the proof explicit.  That is, any u whose conjugate is in the kernel of the transpose of the coefficient matrix corresponds to an outcome that has probability zero.  Also, you don't have transposes in this proof.}

\begin{prop} \label{null space}
Say Alice and Bob share a two-qudit quantum state $\ket{\psi}$. Alice can never observe an outcome corresponding to an end state $\ket{u}$, where $\ket{\Bar{u}} \in Nul(M(\ket{\psi})^T)$.
\end{prop}
\begin{proof}
The shared density matrix is 
\begin{gather*}
\rho=\ket{\psi}\bra{\psi}=(\sum_{i,j} a_{ij}\ket{e_i e_j}(\sum_{i',j'} \overline{a_{i'j'}}\bra{e_{i'} e_{j'}})=\sum_{i,j} \sum_{i',j'} a_{ij} \overline{a_{i'j'}}\ket{e_i e_j}\bra{e_{i'} e_{j'}}.  
\end{gather*}

Let $\ket{u}=\sum_1^n b_i \ket{e_i}$. Then
\begin{equation*} \tag{$\star$}
    \braket{u|i}=\overline{b_i}, \braket{i|u}=b_i
\end{equation*}

Since $\ket{\Bar{u}} \in Nul(M(\ket{\psi}))$, we have 
\begin{equation*} \tag{$\star \star$}
    \forall i, \sum_{j=1}^n a_{ij} \overline{b_i}=0
\end{equation*}

The probability of Alice observing $\ket{u}$, and Bob observing some $\ket{v}$ is
\begin{eqnarray*}
p(\ket{u}_A, \ket{v}_B)&=&\braket{u v | \rho |u v}\\
&=&\sum_{i,j} \sum_{i',j'} a_{ij} \overline{a_{i'j'}}\braket{u|i}\braket{i'|u}_A \otimes \braket{v|j}\braket{j'|v}_B\\
&=&\sum_i \sum_{i'} \sum_j \sum_{j'}  a_{ij} \overline{a_{i'j'}} \braket{u|i}\braket{i'|u}_A \otimes \braket{v|j}\braket{j'|v}_B\\
&=&(\sum_i \sum_j a_{ij} \overline{b_i})(\sum_{i'} \sum_{j'} \overline{a_{i'j'}} b_{i'})_A \otimes \braket{v|j}\braket{j'|v}_B   \; \; \;  (by (\star))\\ 
&=& 0 \; \; \; (by (\star \star))
\end{eqnarray*}
Thus, Alice can never observe an outcome with a state who's conjugate is in the kernel of $M(\ket{\psi})$.
\end{proof}

In the next section, we examine to what extent Bob can {\emph{use}} his end state to figure out what outcome Alice observed.


%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------
\pagebreak

\textcolor{blue}{Careful with notation.  We use parenthesis not subscripts for the coordinate matrix.  I changed some, but not all.} \textcolor{green}{done}
\section{Entanglement and Measuring}
In the last section we analyzed the way in which Alice's and Bob's end states are correlated when they share a composite state. We now continue with our analysis, but the key point being that Bob doesn't have {\emph{access}} to his state vector without making a  {\emph{measurement}}.  This situation is similar to that of \textbf{Examples \ref{example: orthogonal states for measurements}} and \textbf{\ref{example: non-orthogonal states for measurements}}, only now instead of Alice {\emph{selecting}} a state, now she {\emph{measures}} and observes an outcome with an end state.  Specifically, in our previous setup, Alice chooses a state from an orthonormal basis $\{\ket{\varphi_i}\}_{i=1}^n$ known to both herself and to Bob. She then gives the state to Bob, whose task is to identify the index of that state. Our task is different. Now, Alice and Bob share a known composite 2-qudit state $\ket{\psi}$ and Alice measures with respect to the orthonormal basis. After Alice has made a measurement, the task is for Bob to infer Alice's outcome by measuring his part of the system. By \textbf{Lemma \ref{end state lemma}}, if the transpose of the coefficient matrix $M(\ket{\psi_i})$ preserves orthogonality (and has full rank), Bob can measure with respect to the basis $\{M(\ket{\psi_i})^T \ket{\psi_i}\}_{i=1}^n$, which is automatically orthonormal. Thus, by measuring with respect to {\emph{this orthonormal basis}} he will be able to infer which outcome Alice observed by simply applying the inverse of the transpose of the coefficient matrix to the his end state. 

On the other hand, if the transpose of the coefficient matrix does not preserve orthogonality, Bob cannot determine Alice's outcome with certainty {\emph{even if their shared state is maximally entangled}}.  Thus, perfect correlation between Alice's and Bob's end states is not enough to guarantee that he knows the results of her measurement.  This says that there are two different reasons why  Bob may fail to know the result of Alice's measurement.  The first is that their state may not be maximally entangled, so their end states are not perfectly correlated.  The second, is that even though their end states are perfectly correlated, uncertainty is introduced when Bob tries to {\emph{access}} his end state by measuring.  

\textcolor{blue}{Add small transition}

\textcolor{green}{I think I had that $\ket{\psi}$ must be maximally entangled in the below proposition. Is that not needed? }

\begin{prop} \label{prop: ultimate}
Let $\ket{\psi} \in \mathbb{C}^n \otimes \mathbb{C}^{n}$ be a quantum state representing a composite system of Alice and Bob. Say Alice makes a measurement with respect to a basis known to both herself and Bob.  If the transpose of the coefficient matrix preserves orthogonality, then Bob can determine Alice's measurement outcome with certainty. 
\end{prop}

\begin{proof}
This result follows from the calculations in the proof of \textbf{Proposition \ref{entanglement-rank}}, and the fact that a matrix which preserves orthogonality must be of full rank.
\end{proof}
The next Proposition characterizes matrices that preserve orthogonality.

\textcolor{blue}{Combine the next Prop and Lemma into one Proposition with no reference to a coordinate matrix.  That is M is a scalar multiple of a unitary matrix if and only if M is an invertible matrix which preserves orthogonality.  Moreover, if M preserves orthogonality, then M is identical zero, or M is invertible} \textcolor{green}{done}

\begin{prop} \label{orthogonality preserving character}
\textcolor{green}{A nonzero square matrix M is a scalar multiple of a unitary matrix if and only if M is an invertible matrix which preserves orthogonality. Moreover, if M preserves orthogonality, then M is either invertible or a zero matrix.}
\end{prop}

\begin{proof}
First prove the second half of the Proposition by contraposition: any nonzero singular square matrix does not preserve orthogonality. In other words, for an arbitrary nonzero singular $n \times n$ matrix M, there exist $\ket{u}, \ket{v} \in \mathbb{C}^n$ such that $\braket{u|v}=0$ but $\braket{Mu|Mv} \ne 0$.

Let the dimension of $ker(M)$ be m, and let $\{\ket{x_1}, \ket{x_2},\hdots, \ket{x_m}\}$ be a basis of $ker(M)$. Extend that basis to $\{\ket{x_1}, \ket{x_2},\hdots, \ket{x_m}, \hdots, \ket{x_n}\}$. Let
\begin{gather*}
    \ket{u}=\ket{x_1}+\ket{x_n}\\
    \ket{v}=\ket{x_1}-\ket{x_n}
\end{gather*}

Then $\braket{u|v}=(\bra{x_1}+\bra{x_n})(\ket{x_1}-\ket{x_n})=\braket{x_1|x_1}+\braket{x_n|x_1}-\braket{x_1|x_n}-\braket{x_n|x_n}=1+0-0-1=0$.

Since $\ket{x_1} \in ker(M)$, we also have
\begin{gather*}
    \ket{Mu}=M\ket{x_1}+M\ket{x_n}=M\ket{x_n}\\
    \ket{Mv}=M\ket{x_1}-M\ket{x_n}=-M\ket{x_n}    
\end{gather*}

Since $\ket{x_n} \notin ker(M)$, we have $\ket{\sigma}=M\ket{x_n} \ne 0$. So $\braket{Mu|Mv}=-\braket{\sigma|\sigma} \ne 0$. There exist $\ket{u}, \ket{v}$ whose orthogonality is not preserved by the matrix M, so the second half of the Proposition is true.

\bigskip
Now let's prove the first half of the Proposition.

Consider an arbitrary square matrix M that is a scalar multiple of a unitary matrix. Say $M=\lambda U$, where $\lambda$ is a scalar and $U$ is a unitary matrix.
Then if $\braket{u|v}=0$, we have
$$\braket{M(\ket{\psi}u|M(\ket{\psi}v}=\braket{\lambda Uu|\lambda Uv}=\lambda^2 \braket{u|(U^\dagger U)v}=\lambda^2 \braket{u|v}=0.$$ So orthogonality is preserved.

\bigskip
Let's prove the other direction. Say $\braket{Mu|Mv}=0$ when $\braket{u|v}=0$. Since M is a nonzero matrix, by the second half of the Proposition we just prvoed, M must be a nonzero invertible square matrix. 

Fix an arbitrary $v \in \mathbb{C}^n, v \ne 0$.\\ Since M is a nonzero invertible matrix, $M^\dagger M v \ne 0$.

$\forall u \in \mathbb{C}^n$, if $\braket{u|v}=0$, then $\braket{Mu|Mv}=0$, so $\braket{u|M^\dagger Mv}=0$.

So $\forall u \in v^\perp, u \in (M^\dagger Mv)^\perp$. So $v^\perp \subseteq (M^\dagger Mv)^\perp$.

Since $v^\perp$ and $(M^\dagger Mv)^\perp$ both have dimension $n-1$, $v^\perp = (M^\dagger Mv)^\perp$.

So $\exists \lambda_v$ such that $\lambda_v v = M^\dagger M v$. We want to show that regardless of what v we pick initially, $\lambda_v$ stays the same. In other words, if we fix $v' \in \mathbb{C}^n, v' \ne 0, \lambda_{v'} v' = M^\dagger M v', \lambda_v = \lambda_{v'}$
\begin{gather*}
\braket{v|v'}-\braket{v|v'}=0\\
\Rightarrow \braket{M^\dagger M v|v'}-\braket{M^\dagger M v|v'}=0\\
\Rightarrow \braket{M^\dagger M v|v'}-\braket{v|M^\dagger M v'}=0\\
\Rightarrow \lambda_v \braket{v|v'}-\lambda_{v'} \braket{v|v'}=0\\
\Rightarrow (\lambda_v -\lambda_{v'}) \braket{v|v'}=0\\
\Rightarrow  \lambda_v = \lambda_{v'} if \braket{v|v'} \ne 0
\end{gather*}

If $\braket{v|v'}=0$, $\exists \ket{v''}$ such that $\braket{v|v''} \ne 0$ and $\braket{v'|v''} \ne 0$. Then $\lambda_v =\lambda_{v''}, \lambda_{v'}=\lambda_{v''}$. So $\lambda_v=\lambda_{v'}$ even when $\braket{v|v'}=0$.

So for an arbitrary $\ket{v} \in \mathbb{C}^n, \ket{v} \ne 0$, $\exists \lambda$ that's independent of $\ket{v}$ such that $M^\dagger M v = \lambda v$. So $M^\dagger M=\lambda \mathbb{I}_n\Rightarrow \frac{1}{\sqrt{\lambda}}M^\dagger \frac{1}{\sqrt{\lambda}} M=\mathbb{I}_n\Rightarrow M$ is a scalar multiple of a unitary matrix.
\end{proof}



\textcolor{green}{The coefficient matrix of any quantum state is a nonzero square matrix. By \textbf{Lemma \ref{end state lemma}} and \textbf{Proposition \ref{orthogonality preserving character}}, we learn that after Alice has made her measurement with respect to an orthonormal basis ${\ket{\varphi}_i}_{i=1}^n$and yields an outcome $\ket{\varphi}$, Bob will only be able to actually measure with respect to an orthonormal basis $M(\ket{\psi})^T {\ket{\varphi}_i}_{i=1}^n$ if the transpose of the coefficient matrix $M(\ket{\psi})^T$ is a scalar multiple of a unitary matrix. For example, if Alice measured her part of the shared pair of entangled particles with respect to a basis $\{\ket{u}, \ket{v}\}$, then }
Bob can measure his part with respect to the basis $\{M(\ket{\psi})^T\ket{u}, M(\ket{\psi})^T\ket{v}\}$. When $M(\ket{\psi})^T$ is a scalar multiple of a unitary matrix, it is also invertible and hence has full rank. \textcolor{green}{Therefore, the initial 2-qudit state that Alice and Bob shared $\ket{\psi}$ is maximally entangled, implying that the state of Bob's system contains perfect information about Alice according to \textbf{Proposition \ref{entanglement-rank}}. Therefore, by measuring his system with respect to $\{M(\ket{\psi})^T\ket{u}, M(\ket{\psi})^T\ket{v}\}$, Bob can infer Alice's outcome that is either $\ket{u}$, or $\ket{v}$ by applying \textbf{Lemma \ref{end state lemma}}}. 
% Therefore, Bob can infer Alice's measured outcome once he knows which specific basis Alice picked.

Otherwise, Bob can only measure with respect to some basis $\{M(\ket{\psi})^T\ket{u}, (M(\ket{\psi})^T\ket{u})^\perp \}$ and can only infer something meaningful of Alice within certain probability.

\begin{example}
Consider a two-qubit joint state to be $\ket{\psi}=\frac{1}{\sqrt{2}}(\ket{00}+\ket{11})$. Say Alice chooses to measure on the standard basis $\{\ket{0}, \ket{1}\}$.

\textcolor{green}{$M(\ket{\psi})=\frac{1}{\sqrt{2}}\mathbb{I}$, so $M(\ket{\psi})^T=M(\ket{\psi})$, which is obviously a scalar multiple of the unitary matrix $\mathbb{I}$. So by \textbf{Proposition \ref{orthogonality preserving character}} and \textbf{Proposition \ref{prop: ultimate}}, we know Bob can determine Alice's measurement outcome with certainty. This is because he can measure with respect to  $\{M(\ket{\psi})^T\ket{0}, M(\ket{\psi})^T\ket{1}\}=\{\ket{1}, \ket{0}\}$. By Lemma \ref{end state lemma}, after Alice's measurement and Bob's measurement on their own system both with respect to the standard basis, if Bob observes the outcome $\ket{1}$ on his system, he can infer with certainty that Alice observes $\ket{0}$. If he observes the outcome $\ket{1}$ instead, he can infer that Alice observes $\ket{1}$.}
\end{example}

\textcolor{green}{Let's call all 2-qudit quantum states that allow Bob to determine Alice's measurement outcome with certainty \emph{"good"} states. More explicitly, the transpose of the coefficient matrix of a \emph{"good"} must have full rank and is a scalar multiple of a unitary matrix. For a 2-qudit states in $\mathbb{C}^2 \otimes \mathbb{C}^2$, there is an elegant characterization of all such \emph{"good"} states.} 
\begin{corollary}
$\ket{\psi}=\frac{1}{\sqrt{2}}(a\ket{00}+(-e^{i\theta})\Bar{b}\ket{01}+b\ket{10}+e^{i\theta}\Bar{a}\ket{11})$, where $|a|^2+|b|^2=1$ is a characterization of all â€œgood" states in $\mathbb{C}^2 \otimes \mathbb{C}^2$ such that if $\braket{u|v}=0$, then $\braket{M(\ket{\psi})^Tu|M(\ket{\psi})^Tv)}=0$.
\end{corollary}

\begin{example}
Consider a two-qubit state not within the characterization in Proposition \ref{orthogonality preserving character} $\ket{\psi}=\frac{1}{\sqrt{3}}(\ket{00}+\ket{01}+\ket{10})$. Say Alice chooses to measure on the standard basis $\{\ket{0}, \ket{1}\}$.

Bob will not be able to measure on $\{M(\ket{\psi})\ket{0}, M(\ket{\psi})\ket{1}\}=\{\begin{pmatrix}
\frac{1}{\sqrt{3}}\\
\frac{1}{\sqrt{3}}
\end{pmatrix},\begin{pmatrix}
\frac{1}{\sqrt{3}}\\
0
\end{pmatrix}\}$ because it's not orthogonal. So he will only be able to choose to measure on $\{M(\ket{\psi})\ket{0}, (M(\ket{\psi})\ket{0})^\perp \}$ or $\{(M(\ket{\psi})\ket{1})^\perp, M(\ket{\psi})\ket{1} \}$.
Without loss of generality, let's say Bob chooses to measure on $\{M(\ket{\psi})\ket{0}, (M(\ket{\psi})\ket{0})^\perp \}=\{\begin{pmatrix}
\frac{1}{\sqrt{3}}\\
\frac{1}{\sqrt{3}}
\end{pmatrix}, \begin{pmatrix}
\frac{1}{\sqrt{3}}\\
-\frac{1}{\sqrt{3}}
\end{pmatrix}\}$.
We normalize the two vectors and call them $\ket{u}'$ and $\ket{v}'$, respectively. So the orthonormal basis for Bob is $\{\ket{u'}, \ket{v'}\}$, where $\ket{u}'=\begin{pmatrix}
\frac{1}{\sqrt{2}}\\
\frac{1}{\sqrt{2}}
\end{pmatrix}$, $\ket{v}'=\begin{pmatrix}
\frac{1}{\sqrt{2}}\\
-\frac{1}{\sqrt{2}}
\end{pmatrix}$. Therefore to Bob, if he measures with basis $\ket{u'}, \ket{v'}$ and yields $\ket{v'}$, he can infer with certainty that Alice observes $\ket{1}$. However, if Bob yields $\ket{u'}$, then he is not able to tell what Alice observes.
\end{example}
%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------
\pagebreak

\textcolor{blue}{Need to add some here.  Also maybe add a diagram or a series of diagram} \textcolor{green}{what kind of diagram?}

\section{Application to one-time pad}
The above classification of entanglement can potentially be used in the following cryptographic scheme.
\begin{enumerate}
    \item Bob creates a series of maximally entangled (as defined in \textbf{Definition \ref{def: maximally entangled}}) 2-qudit states $L={\ket{\psi_1}, \ket{\psi_2}, \hdots, \ket{\psi_m}}$, where $\ket{\psi_i} \in \mathbb{C}^n \otimes \mathbb{C}^n$. He sends the left qudit of each state to Alice via a quantum communication channel and keep the right side to himself.
    \item Via a classical communication channel that doesn't have to be secure, Alice notify Bob in order the set of orthonormal bases that she is going to make measurements with. In other words, for each $\ket{\psi_i}$, Alice randomly chooses an orthonormal basis $B_i=\{\ket{b_1}, \ket{b_2}, \hdots, \ket{b_n}\}$ and sends the elements in the basis in order to Bob.
    \item For each qudit received from L, Alice makes a measurement and saves her result locally. With the knowledge of L and $B_1, B_2, \hdots, B_m$, Bob will be able to infer Alice's measurement result by measuring with respect to basis $\{M(\ket{\psi_i})^T \ket{b_1}, M(\ket{\psi_i})^T \ket{b_2}, \hdots, M(\ket{\psi_i})^T \ket{b_n}\}$. He saves the result for each $\ket{\psi_i}$ and in the end get an m-digit long key that can be used for one-time pad introduced in \textbf{Definition \ref{def: quantum one-time pad}}.
\end{enumerate}